{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a106c2-f185-492f-87bb-79286c2eacc4",
   "metadata": {},
   "source": [
    "### Business Analytics Group Assignment - Predicting Airbnb Listing Prices in Melbourne__ {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c316658-edf7-4797-b078-202987b4922b",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "**Kaggle Competition Ends:** Friday, 2 June 2023 @ 3:00pm (Week 13)  \n",
    "**Assignment Due Date on iLearn:** Friday, 2 June 2023 @ 11.59pm (Week 13)   \n",
    "\n",
    "**Overview:**   \n",
    "\n",
    "- In the group assignment you will form a team of 3 students and participate in a forecasting competition on Kaggle\n",
    "- The goal is to predict listed prices of Airbnb properties in Melbourne based on various Airbnb characteristics and regression models\n",
    "- Assessment Summary:  \n",
    "    - Write a problem statement and perform Exploratory Data Analysis  \n",
    "    - Clean up data, deal with categorical features and missing observations, and create new explanatory variables (feature engineering)  \n",
    "    - Construct and tune forecasting models, produce forecasts and submit your predictions to Kaggle  \n",
    "    - Each member of the team will record a video presentation of their work  \n",
    "    - Marks will be awarded producing a prediction in the top 5 positions of their unit as well as for reaching the highest ranking on Kaggle amongst all teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2270744-5e12-447b-a861-9719409e287c",
   "metadata": {},
   "source": [
    "**Instructions:** \n",
    "\n",
    "- Form a team of 3 students (minimum 2 students)  \n",
    "- Each team member needs to join [https://www.kaggle.com](https://www.kaggle.com/)  \n",
    "- Choose a team leader and form a team in the competition [https://www.kaggle.com/t/a154f28787174b628a2b7eaa238a5c12](https://www.kaggle.com/t/a154f28787174b628a2b7eaa238a5c12)\n",
    "    - Team leader to click on `team` and join and invite other team members to join\n",
    "    - Your **team's name must start** with your unit code, for instance you could have a team called BUSA8001_masterful_geniuses or BUSA3020_l33t \n",
    "- All team members should work on all the tasks listed below however   \n",
    "    - Choose a team member who will be responsible for one of each of the 3 tasks listed below    \n",
    "- Your predictions must be generated by a model you develop here \n",
    "    - You will receive a mark of zero if your code provided here does not produce the forecasts you submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ee56b-c5b9-4078-8754-e04f90797514",
   "metadata": {},
   "source": [
    "**Marks**: \n",
    "\n",
    "- Total Marks: 40\n",
    "- Your individual mark will consist of:  \n",
    "    - 50% x overall assignment mark + 45% x mark for the task that you are responsible for + 5% x mark received from your teammates for your effort in group work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f70e4c-b794-4555-b170-d80b628b2904",
   "metadata": {},
   "source": [
    "**Competition Marks:**  \n",
    "\n",
    "- 1 mark: Ranking in the top 5 places of your unit on Kaggle (make sure you name your team as instructed above)   \n",
    "- 2 marks: Reaching the first place in your unit (make sure you name your team as instructed above)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3180e9-c3cc-494c-ae64-d3b9701e10e8",
   "metadata": {},
   "source": [
    "\n",
    "**Submissions:**  \n",
    "\n",
    "1. On Kaggle: submit your team's forecast in order to be ranked by Kaggle\n",
    "    - Can do this as many times as necessary while building their model  \n",
    "2. On iLearn **only team leader to submit** this Jupyter notebook re-named `Group_Assignment_Team_Name.ipynb` where Team_Name is your team's name on Kaggle   \n",
    "    - The Jupyter notebook must contain team members names/ID numbers, and team name in the competition\n",
    "    - Provide answers to the 3 Tasks below in the allocated cells including all codes/outputs/writeups \n",
    "    - One 15 minute video recording of your work \n",
    "        - Each team member to provide a 5 minute presentation of the Task that they led (it is best to jointly record your video using Zoom)\n",
    "        - When recording your video make sure your face is visible, that you share your Jupyter Notebook and explain everything you've done in the submitted Jupyter notebook on screen\n",
    "        - 5 marks will be deducted from each Task for which there is no video presentation or if you don't follow the above instructions\n",
    "        \n",
    "3. On iLearn each student needs to submit a file with their teammates' names, ID number and a mark for their group effort (out of 100%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbce8a-bf4f-4155-9336-6367fd239f6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe68a60-5e8b-4c4e-b562-3a82fa426395",
   "metadata": {},
   "source": [
    "**Fill out the following information**\n",
    "\n",
    "For each team member provide name, Student ID number and which task is performed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac487d3-755a-4a69-a22f-d599dbdd7979",
   "metadata": {},
   "source": [
    "- Team Name on Kaggle: `BUSA8001_superhosts`\n",
    "- Team Leader and Team Member 1: `Felix Rosenberger`\n",
    "- Team Member 2: `John Rizk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24de689-928d-4991-b337-760c12780e5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Problem Description and Initial Data Analysis {-}\n",
    "\n",
    "1. Read the Competition Overview on Kaggle [https://www.kaggle.com/t/a154f28787174b628a2b7eaa238a5c12](https://www.kaggle.com/t/a154f28787174b628a2b7eaa238a5c12)\n",
    "2. Referring to Competition Overview and the data provided on Kaggle write about a 500 words **Problem Description** focusing on key points that will need to be addressed as first steps in Tasks 2 and 3 below, using the following headings:\n",
    "    - Forecasting Problem - explain what you are trying to do and how it could be used in the real world (i.e. why it may be important)\n",
    "    - Evaluation Criteria - explain the criteria is used to assess forecast performance \n",
    "    - Types of Variables/Features\n",
    "    - Data summary and main data characteristics\n",
    "    - Missing Values (only explain what you found at this stage)\n",
    "    - Hint: you should **not** discuss any specific predictive algorithms at this stage\n",
    "    - Note: This task should be completed in a single Markdown cell (text box)\n",
    "    \n",
    "Total Marks: 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcfd49e-fcbb-4eaa-936b-144b574495d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in data\n",
    "trainpath = \"train.csv\"\n",
    "df_train = pd.read_csv(trainpath, index_col='ID')\n",
    "\n",
    "testpath = \"test.csv\"\n",
    "df_test = pd.read_csv(testpath, index_col='ID')\n",
    "\n",
    "# concatenate dataframes to reduce redundancies in operations\n",
    "df = pd.concat([df_train, df_test])\n",
    "\n",
    "#df.head()\n",
    "df.to_csv(\"df_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960bb55-92da-4528-9ee1-e06e46f00cae",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Forecasting Problem</font>\n",
    "<font color='darkblue'>\n",
    "Aiirbnb is an \"online marketplace that connects people who want to rent out their homes with people who are looking for accommodations in specific locales.\" https://www.investopedia.com/articles/personal-finance/032814/pros-and-cons-using-airbnb.asp\n",
    "\n",
    "The goal of is this assignment is to develop a model for predicting nightly prices of Melbournd based Airbnb listings with different features and characteristics based on statistical machine learning. The model can be used to assess the how rental prices differ based on specific characteristics of the property or against other suburbs, which can then be used to determine the profitability and feasibilty of certain listings. \n",
    "\n",
    "### Evaluation Criteria\n",
    "The criteria to assess prediction performance is RMSE. This performance metric measures the average distance between predictions obtained by a model and actual target values. Thus, the lower the distance (and the smaller RMSE), the better the prediction quality. It also has the advantage of being in the same unit as the predicted variable which makes it easy to interpret.\n",
    "\n",
    "### Variables / Features\n",
    "\n",
    "The data consists of 60 columns, 27 of type object, 18 of type float64 and 15 of type int64 types.\n",
    "<br>\n",
    "<br>\n",
    "The variables types were classified into the following data types, 21 nominal, 12 ordinal and 27 numeric.\n",
    "\n",
    "\n",
    "### Data Summary and Main Characteristics\n",
    "\n",
    "Evaluation of the prices suggest that the distribution of prices is skewed with the range of prices between <b>25</b> and <b>145160</b> with a mean of <b>285.65</b>.\n",
    "\n",
    "Prices appear to be sensitive to property type, with <b>Private room in villa</b> having the highest mean price of <b>2358.36</b> by <b>property type</b>, which is much higher than the mean prices of other property types, suggesting that the distribution of prices for this property type may be highly skewed. <b>Private room in bungalow</b> has the lowest mean price of <b>64.11</b>. \n",
    "\n",
    "<b>Entire rental unit</b> have the highest listings <b>2984</b>, and a mean price of <b>296.87</b>, which is close to the overall mean price, while <b>Shared room in guesthouse</b> has the least listings <b>2</b>, and a mean price of <b>67.00</b>.\n",
    "\n",
    "<b>Boroondara</b> has the highest mean price of <b>894.95</b> by <b>neighbourhood_cleansed</b> and <b>Greater Dandenong</b> has the lowest mean price of <b>115.41</b>. Prices also seem sensitive to neighbourhood_cleansed. <b>Melbourne</b> has the highest listing of <b>2062</b>, and mean price of <b>335.35</b>, and <b>Greater Dandenong</b> has the lowest lisitngs of<b>32</b>.\n",
    "\n",
    "<b>Instant bookable</b> properties have a slighly higher mean price <b>298.96</b> compared with those that are not instant bookable <b>281.34</b>.\n",
    "\n",
    "<b>Entire home/apt</b> have the highest listing <b>by room type</b> with a mean price of <b>312.19</b>. <b>Hotel room</b> and <b>Shared room</b> have the least lisitings with a combined total of <b>80</b>.\n",
    "\n",
    "Prices increase as the number of <b>accomodates</b> increases from 1 to 16, ranging from a mean of <b>81.65</b> to <b>724.04</b>. \n",
    "\n",
    "Intial review of the features indicates that price dependant variables may include:\n",
    "<br>&nbsp;&nbsp;room_type (nominal)\n",
    "<br>&nbsp;&nbsp;neighbourhood_cleansed (nominal)\n",
    "<br>&nbsp;&nbsp;accommodates (numeric)\n",
    "<br>&nbsp;&nbsp;bathrooms (numeric)\n",
    "<br>&nbsp;&nbsp;bedrooms (numeric)\n",
    "<br>&nbsp;&nbsp;beds (numeric)\n",
    "<br>&nbsp;&nbsp;amenities (nominal)\n",
    "<br>&nbsp;&nbsp;review_scores_rating (ordinal)\n",
    "<br>&nbsp;&nbsp;instant_bookable (nominal)\n",
    "\n",
    "### Missing Observation\n",
    "\n",
    "There are 24202 missing values across 29 variables, spread fairly evenly between the train and data sets.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30df24-b18b-4f61-975f-ceebe2e2c3ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Data Cleaning, Missing Observations and Feature Engineering {-}\n",
    "- In this task you will follow a set of instructions/questions listed below.\n",
    "- Make sure you **explain** each step you do both in Markdown text and on your video.\n",
    "    - Do not just read out your commands without explaining what they do and why you used them \n",
    "\n",
    "Total Marks: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4ec3d-4936-4428-a204-04f133210a3d",
   "metadata": {},
   "source": [
    "**Task 2, Question 1**: Clean **all** numerical features and the target variable `price` so that they can be used in training algorithms. For instance, `host_response_rate` feature is in object format containing both numerical values and text. Extract numerical values (or equivalently eliminate the text) so that the numerical values can be used as a regular feature.  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32485e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Data Cleaning\n",
    "\n",
    "#Functions\n",
    "def replace_string(df, c, s, r='', f='strip'):\n",
    "    if f == 'find_replace':\n",
    "        mask = (df[c].notnull()) & (df[c].astype(str).str.contains(s))\n",
    "        df.loc[mask, c] = df.loc[mask, c].astype(str).str.replace(s, r)\n",
    "    if f == 'replace':\n",
    "        df[c] = df[c].replace(s, r)\n",
    "    elif f == 'strip':\n",
    "        df[c] = df[c].dropna().astype(str).str.replace(s, r, regex=True)\n",
    "    return df\n",
    "\n",
    "def replace_numeric(df, c, n, r=0, f='match'):\n",
    "    if f == 'isgreater':\n",
    "        df.loc[df[c] > n, c] = r\n",
    "    elif f == 'isless':\n",
    "        df.loc[df[c] < n, c] = r\n",
    "    elif f == 'match':\n",
    "        df.loc[df[c] == n, c] = r\n",
    "    return df\n",
    "\n",
    "def convert_numeric(df, c, t, d=1):\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df[c] = df[c].astype(t)\n",
    "    df[c] = df[c] / d\n",
    "    return df\n",
    "\n",
    "df2 = df\n",
    "\n",
    "\n",
    "# price\n",
    "df2 = replace_string(df2, 'price', '$','', 'strip')\n",
    "df2 = replace_string(df2, 'price', ',','', 'strip')\n",
    "df2 = convert_numeric(df2, 'price', 'float', 1)\n",
    "\n",
    "# host_response_rate\n",
    "df2 = replace_string(df2, 'host_response_rate', '%','', 'strip')\n",
    "df2 = convert_numeric(df2, 'host_response_rate', 'float', 100)\n",
    "\n",
    "# host_acceptance_rate\n",
    "df2 = replace_string(df2, 'host_acceptance_rate', '%','', 'strip')\n",
    "df2 = convert_numeric(df2, 'host_acceptance_rate', 'float', 100)\n",
    "\n",
    "# bathrooms\n",
    "df2 = replace_string(df2, 'bathrooms', 'Half-bath','0.5', 'find_replace')\n",
    "df2 = replace_string(df2, 'bathrooms', 'half-bath','0.5', 'find_replace')\n",
    "df2 = replace_string(df2, 'bathrooms', '[^0-9\\.]','', 'strip')\n",
    "df2 = convert_numeric(df2, 'bathrooms', 'float', 1)\n",
    "\n",
    "# max/min nights - replace extreme values\n",
    "df2 = replace_numeric(df2, 'maximum_nights', 9000, 1000, 'isgreater')\n",
    "df2 = replace_numeric(df2, 'minimum_maximum_nights', 9000, 1000, 'isgreater')\n",
    "df2 = replace_numeric(df2, 'maximum_maximum_nights', 9000, 1000, 'isgreater')\n",
    "df2 = replace_numeric(df2, 'minimum_nights_avg_ntm', 9000, 1000, 'isgreater')\n",
    "df2 = replace_numeric(df2, 'maximum_nights_avg_ntm', 9000, 1000, 'isgreater')\n",
    "\n",
    "df2.to_csv(\"df_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313fef0-4043-4c03-8f3f-4691b5f61f11",
   "metadata": {},
   "source": [
    "#### <font color='darkblue'>price</font>\n",
    "<font color='darkblue'>\n",
    "The target variable price was converted from a string containing the $ sign to a numeric float variable.\n",
    "<br>\n",
    "    \n",
    "#### <font color='darkblue'>host_response_rate and host_acceptance_rate</font>    \n",
    "host_response_rate and host_acceptance_rate variables were converted from strings containing the % sign to a numeric float variable.\n",
    "<br>\n",
    "\n",
    "#### <font color='darkblue'>bathrooms</font>\n",
    "Some records for bathrooms were 'Half-bath' and 'half-bath', that is they did not contain any numeric characters. These were converted to the string '0.5'. For the remaining records non-numeric characters were removed and then the whole column was converted to a numeric float variable.\n",
    "<br>\n",
    "\n",
    "#### <font color='darkblue'>maximum_nights, minimum_maximum_nights,maximum_maximum_nights and maximum_nights_avg_ntm</font>    \n",
    "maximum_nights, minimum_maximum_nights,maximum_maximum_nights and maximum_nights_avg_ntm had some extreme values, which appear to be in error - these were replaced with 1000 which appears to be the threshold based on other min/max nights variables.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f1bb1",
   "metadata": {},
   "source": [
    "**Task 2, Question 2** Create at least 4 new features from existing features which contain multiple items of information, e.g. creating `email`,  `phone`, `work_email`, etc. from feature `host_verifications`.  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d210de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2\n",
    "\n",
    "df3 = df2\n",
    "\n",
    "# Create new features email, phone and work_email from host_verifications\n",
    "df3 = replace_string(df3, 'host_verifications', \"['email']\",\"'1','0','0'\", 'replace')\n",
    "df3 = replace_string(df3, 'host_verifications', \"['phone']\",\"'0','1','0'\", 'replace')\n",
    "df3 = replace_string(df3, 'host_verifications', \"['email', 'phone']\",\"'1','1','0'\", 'replace')\n",
    "df3 = replace_string(df3, 'host_verifications', \"['phone', 'work_email']\",\"'0','1','1'\", 'replace')\n",
    "df3 = replace_string(df3, 'host_verifications', \"['email', 'phone', 'work_email']\",\"'1','1','1'\", 'replace')\n",
    "\n",
    "df3[['email', 'phone', 'work_email']] = df3['host_verifications'].str.split(',', expand=True)\n",
    "\n",
    "df3 = replace_string(df3, 'email', \"'\",'', 'strip')\n",
    "df3 = convert_numeric(df3, 'email','int', 1)\n",
    "\n",
    "df3 = replace_string(df3, 'phone', \"'\",'', 'strip')\n",
    "df3 = convert_numeric(df3, 'phone','int', 1)\n",
    "\n",
    "df3 = replace_string(df3, 'work_email', \"'\",'', 'strip')\n",
    "df3 = convert_numeric(df3, 'work_email','int', 1)\n",
    "\n",
    "df3.drop(['host_verifications'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e3e114-11e1-4178-ba88-1de4f5271ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new features smoke_alarm, kitchen, essential, hangers, wifi from amenities\n",
    "# These are the top 5 ammenities in the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "amenity_count = Counter()\n",
    "amenity_count_total = Counter()\n",
    "count_total = []\n",
    "\n",
    "for amenities_str in df3['amenities']:\n",
    "    amenity_count_total = 0\n",
    "    amenities_list = amenities_str.strip('][').replace('\"', '').split(', ')\n",
    "    for amenity in amenities_list:\n",
    "        amenity_count[amenity] += 1\n",
    "        amenity_count_total  += 1\n",
    "    count_total.append(amenity_count_total)\n",
    "\n",
    "df_amenities = pd.DataFrame(columns=[ 'amenity_count'])\n",
    "df_amenities['amenity_count'] = amenity_count\n",
    "df_amenities = df_amenities.sort_values('amenity_count', ascending=False)\n",
    "df_amenities.head(5)\n",
    "\n",
    "#df_acc = pd.DataFrame(columns=[ 'total_amenity_counts'])\n",
    "#df_acc['total_amenity_counts'] = count_total\n",
    "#df_acc\n",
    "#print(acc)\n",
    "\n",
    "#Smoke alarm\t9548\n",
    "#Kitchen\t9383\n",
    "#Essentials\t9327\n",
    "#Hangers\t8702\n",
    "#Wifi\t8618\n",
    "\n",
    "df3['amenity_count'] = count_total\n",
    "\n",
    "df3[['smoke_alarm','kitchen','essentials','hangers','wifi']] = 0\n",
    "\n",
    "for idx, amenities_str in df3['amenities'].items():\n",
    "    amenities_list = amenities_str.strip('][').replace('\"', '').split(', ')\n",
    "    if 'Smoke alarm' in amenities_list:\n",
    "        df3.loc[idx, 'smoke_alarm'] = 1\n",
    "    if 'Kitchen' in amenities_list:\n",
    "        df3.loc[idx, 'kitchen'] = 1        \n",
    "    if 'Essentials' in amenities_list:\n",
    "        df3.loc[idx, 'essentials'] = 1      \n",
    "    if 'Hangers' in amenities_list:\n",
    "        df3.loc[idx, 'hangers'] = 1      \n",
    "    if 'Wifi' in amenities_list:\n",
    "        df3.loc[idx, 'wifi'] = 1              \n",
    "\n",
    "\n",
    "df3.drop(['amenities'], axis=1, inplace=True)\n",
    "\n",
    "df3.to_csv(\"df_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fde9b6-bed0-4018-9edb-3553f3d1fa13",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>New Features</font>\n",
    "<font color='darkblue'>\n",
    "\n",
    "#### <font color='darkblue'>host_verifications</font>    \n",
    "Created new binary numeric features 'email', 'phone', 'work_email from 'host_verifications' and deleted the column 'host_verifications'\n",
    "<br>\n",
    "    \n",
    "#### <font color='darkblue'>amenities</font>    \n",
    "Create new binary numeric features 'smoke_alarm', 'kitchen', 'essential', 'hangers', 'wifi' from 'amenities' which are the top 5 amenities in the dataset and deleted the column 'amenities'.\n",
    "<br>\n",
    "</font>### <font color='darkblue'>New Features</font>\n",
    "<font color='darkblue'>\n",
    "\n",
    "#### <font color='darkblue'>host_verifications</font>    \n",
    "Created new binary numeric features 'email', 'phone', 'work_email from 'host_verifications' and deleted the column 'host_verifications'\n",
    "<br>\n",
    "    \n",
    "#### <font color='darkblue'>amenities</font>    \n",
    "Create new binary numeric features 'smoke_alarm', 'kitchen', 'essential', 'hangers', 'wifi' from 'amenities' which are the top 5 amenities in the dataset and deleted the column 'amenities'.\n",
    "<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d01c4d-1412-4e51-8f80-7040f536b1c7",
   "metadata": {},
   "source": [
    "**Task 2, Question 3**: Impute missing values for all features in both training and test datasets. Hint: make sure you do **not** impute the price in the test dataset.\n",
    "(3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36412d68-f68f-4764-aa6f-88ba33116ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing(df, c, s='most_frequent'):\n",
    "    for col in c:\n",
    "        i = SimpleImputer(missing_values = np.nan, strategy=s) \n",
    "        i = i.fit(df[[col]])\n",
    "        df[[col]] = i.transform(df[[col]])\n",
    "    return df\n",
    "\n",
    "# host_location-> most_frequent\n",
    "df4 = impute_missing(df4, ['host_location'], 'most_frequent')\n",
    "df4 = df3\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing(df, c, s='most_frequent'):\n",
    "    for col in c:\n",
    "        i = SimpleImputer(missing_values = np.nan, strategy=s) \n",
    "        i = i.fit(df[[col]])\n",
    "        df[[col]] = i.transform(df[[col]])\n",
    "    return df\n",
    "\n",
    "# host_location-> most_frequent\n",
    "df4 = impute_missing(df4, ['host_location'], 'most_frequent')\n",
    "\n",
    "# host_response_time -> most_frequent\n",
    "df4 = impute_missing(df4, ['host_response_time'], 'most_frequent')\n",
    "\n",
    "# host_response_rate, host_acceptance_rate -> mean\n",
    "df4 = impute_missing(df4, ['host_response_rate', 'host_acceptance_rate'], 'mean')\n",
    "\n",
    "# host_is_superhost -> most_frequent\n",
    "df4 = impute_missing(df4, ['host_is_superhost'], 'most_frequent')\n",
    "\n",
    "# host_neighbourhood, neighbourhood, neighbourhood_cleansed -> most_frequent\n",
    "df4 = impute_missing(df4, ['host_neighbourhood', 'neighbourhood', 'neighbourhood_cleansed'], 'most_frequent')\n",
    "\n",
    "# property_type, room_type -> most_frequent\n",
    "df4 = impute_missing(df4, ['property_type', 'room_type'], 'most_frequent')\n",
    "\n",
    "# bathrooms, bedrooms, beds, first_review -> median\n",
    "df4 = impute_missing(df4, ['bathrooms','bedrooms','beds'], 'median')\n",
    "\n",
    "# minimum_minimum_nights, maximum_maximum_nights -> median\n",
    "df4 = impute_missing(df4, ['minimum_minimum_nights', 'maximum_maximum_nights'], 'median')\n",
    "\n",
    "# availability_365 -> mean\n",
    "df4 = impute_missing(df4, ['availability_365'], 'mean')\n",
    "\n",
    "# first_review, last_review -> most_frequent\n",
    "df4 = impute_missing(df4, ['first_review', 'last_review'], 'most_frequent')\n",
    "\n",
    "#review_scores_accuracy, review_scores_checkin, review_scores_cleanliness, review_scores_communication, review_scores_location\n",
    "# review_scores_rating, review_scores_value -> mean\n",
    "df4 = impute_missing(df4, ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness','review_scores_checkin',\n",
    "                           'review_scores_communication', 'review_scores_location','review_scores_value'], 'mean')\n",
    "# reviews_per_month -> mean\n",
    "df4 = impute_missing(df4, ['reviews_per_month'], 'mean')\n",
    "\n",
    "# email, phone, work_email from -> most_frequent\n",
    "df4 = impute_missing(df4, ['email', 'phone', 'work_email'], 'most_frequent')\n",
    "\n",
    "# smoke_alarm, kitchen, essentials, hangers, wifi -> most_frequent\n",
    "df4 = impute_missing(df4, ['smoke_alarm', 'kitchen', 'essentials', 'hangers', 'wifi'], 'most_frequent')\n",
    "\n",
    "\n",
    "df4.to_csv(\"df_4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb16b1d-f4f1-438e-a5af-ae9cc3c04467",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Imputing missing values</font>\n",
    "<font color='darkblue'>\n",
    "<br>\n",
    "Imputed missing values for all features except for 'description', 'neighborhood_overview', 'host_location' and 'host_about'.\n",
    "<br>\n",
    "<br>    ### <font color='darkblue'>Imputing missing values</font>\n",
    "<font color='darkblue'>\n",
    "<br>\n",
    "Imputed missing values for all features except for 'description', 'neighborhood_overview', 'host_location' and 'host_about'.\n",
    "<br>\n",
    "<br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c14ab6-aff9-4fcd-ab7b-34f0b77f8caa",
   "metadata": {},
   "source": [
    "**Task 2, Question 4**: Encode all categorical variables appropriately as discussed in class. \n",
    "\n",
    "\n",
    "Where a categorical feature contains more than 5 unique values, map the feature into 5 most frequent values + 'other' and then encode appropriately. For instance, you could group then map `property_type` into 5 most frequent property types + 'other'  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f91f4448-8574-4397-a636-d83bbde885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4\n",
    "\n",
    "# onehot encoder function\n",
    "def onehot(df, c):\n",
    "    for col in c:\n",
    "        df = df.join(pd.get_dummies(df[[col]], drop_first=True))\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# encode binary classifiers\n",
    "# 'host_is_superhost','host_has_profile_pic','host_identity_verified','has_availability','instant_bookable'\n",
    "df5 = onehot(df5, ['source', 'host_is_superhost','host_has_profile_pic','host_identity_verified','has_availability','instant_bookable'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# encode source\n",
    "#le = encoder.fit_transform(df5['source'].values)\n",
    "#df5['source'] = le\n",
    "#print('source:', encoder.classes_)\n",
    "\n",
    "# encode room_type\n",
    "le = encoder.fit_transform(df5['room_type'].values)\n",
    "df5['room_type'] = le\n",
    "room_type_classes = encoder.classes_\n",
    "\n",
    "\n",
    "#encode top 5 property_type and other\n",
    "top_5_property_type = df5['property_type'].value_counts().nlargest(5).index.tolist()  \n",
    "encoder.fit(top_5_property_type + ['other'])  \n",
    "#df5['property_type_encoded'] = df5['property_type'].apply(lambda x: x if x in top_5_property_type else 'other')\n",
    "df5['property_type'] = df5['property_type'].apply(lambda x: x if x in top_5_property_type else 'other')\n",
    "df5 = onehot(df5, ['property_type'])\n",
    "\n",
    "#df5['property_type_encoded'] = encoder.transform(df5['property_type'].apply(lambda x: x if x in top_5_property_type else 'other'))\n",
    "#df5.drop(['property_type'], axis=1, inplace=True)\n",
    "#df5 = df5.rename(columns={'property_type_encoded': 'property_type'})\n",
    "#property_type_classes = encoder.classes_\n",
    "\n",
    "\n",
    "#encode top 5 neighbourhood_cleansed and other\n",
    "top_5_neighbourhood_cleansed = df5['neighbourhood_cleansed'].value_counts().nlargest(5).index.tolist()  \n",
    "encoder.fit(top_5_neighbourhood_cleansed + ['other'])  \n",
    "\n",
    "df5['neighbourhood_cleansed'] = df5['neighbourhood_cleansed'].apply(lambda x: x if x in top_5_neighbourhood_cleansed else 'other')\n",
    "df5 = onehot(df5, ['neighbourhood_cleansed'])\n",
    "\n",
    "\n",
    "\n",
    "#df5['neighbourhood_cleansed_encoded'] = encoder.transform(df5['neighbourhood_cleansed'].apply(lambda x: x if x in top_5_neighbourhood_cleansed else 'other'))\n",
    "#df5.drop(['neighbourhood_cleansed'], axis=1, inplace=True)\n",
    "#df5 = df5.rename(columns={'neighbourhood_cleansed_encoded': 'neighbourhood_cleansed'})\n",
    "#neighbourhood_cleansed_classes = encoder.classes_\n",
    "\n",
    "\n",
    "# map/rank host_response_time\n",
    "host_response_mapping = {'within an hour':1, 'within a few hours':2, 'within a day':3, 'a few days or more':4}\n",
    "df5['host_response_time'] = df5['host_response_time'].map(host_response_mapping)\n",
    "\n",
    "# convert host_since into days based on current date\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "df5['host_since'] = pd.to_datetime(df5['host_since'], format='%Y/%m/%d')\n",
    "df5['host_since'] = (today - df5['host_since']).dt.days\n",
    "\n",
    "#df5\n",
    "\n",
    "df5.to_csv(\"df_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad3f20-7549-4b9e-8894-818c976d1457",
   "metadata": {},
   "source": [
    "<font color='darkblue'>\n",
    "\n",
    "* Encoded  binray classifications   \n",
    "'source', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable' using onehot encoding.  \n",
    "\n",
    "* Encoded 'room_type' using LabelEncoder.  \n",
    "* Encoded 'property_type' and 'neighbourhood_cleansed' using LabelEncoder grouped by the top 5 classifications in each and the rese grouped as 'other'  \n",
    "* Encoded and ranked host_response_time.\n",
    "</font>\n",
    "<br><font color='darkblue'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d695347",
   "metadata": {},
   "source": [
    "**Task 2, Question 5**: Perform any other actions you think need to be done on the data before constructing predictive models, and clearly explain what you have done.   \n",
    "(1 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a238a3-5432-4058-8b6e-1172c4e396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df6 = df5\n",
    "df6['log_price'] = np.log(df6['price'])\n",
    "\n",
    "df6.to_csv(\"df_6.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a081c-cdc4-4fc8-9bd7-52bfd4c3e6dd",
   "metadata": {},
   "source": [
    "<font color='darkblue'>\n",
    "\n",
    "Added log_price to normalise price distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16475c",
   "metadata": {},
   "source": [
    "**Task 2, Question 6**: Perform exploratory data analysis to measure the relationship between the features and the target and write up your findings. \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1509b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2, Question 6 Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a965eb-e661-4772-af72-98f19e1bae41",
   "metadata": {},
   "source": [
    "`(Task 2, Question 6 Text Here)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f6968-d9c2-422f-9935-eba856b3c028",
   "metadata": {},
   "source": [
    "--- \n",
    "## Task 3: Fit and tune a forecasting model/Submit predictions/Report score and ranking {-}\n",
    "\n",
    "Make sure you **clearly explain each step** you do, both in text and on the recoded video.\n",
    "\n",
    "1. Build a machine learning (ML) regression model by taking into account the outcomes of Tasks 1 & 2 (Explain carefully)\n",
    "2. Fit the model and tune hyperparameters via cross-validation: make sure you comment and explain each step clearly\n",
    "3. Create predictions using the test dataset and submit your predictions on Kaggle's competition page\n",
    "4. Provide Kaggle ranking and **score** (screenshot your best submission) and Comment\n",
    "5. Make sure your Python code works, so that a marker that can replicate your all of your results and obtain the same RMSE from Kaggle\n",
    "\n",
    "- Hint: to perform well in this assignment you will need to iterate Tasks 2 & 3, creating new features and training various models in order to find the best one.\n",
    "\n",
    "Total Marks: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb53af94-d769-4a1d-a4a1-433f3dc7b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import pcakages/libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "520e10db-169f-4971-a441-0f053e8104e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find and remove outliers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_outliers(data, threshold=3):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "    z_scores = scaled_data.flatten()\n",
    "    outlier_locations = np.where(np.abs(z_scores) > threshold)[0]\n",
    "    outlier_values = data[outlier_locations]\n",
    "    outlier_df = pd.DataFrame({'Outlier': outlier_values})\n",
    "    return outlier_df\n",
    "\n",
    "def find_outliers(newDF: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Find rows containing outliers with IQR\"\"\"\n",
    "    \n",
    "    Q1 = newDF.quantile(0.25)\n",
    "    Q3 = newDF.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lowerLimit = Q1-(1.5*IQR)\n",
    "    upperLimit = Q3+(1.5*IQR)\n",
    "    \n",
    "    return newDF[newDF[(newDF < lowerLimit) | (newDF > upperLimit)].notna().sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c90fa9-86d2-4ae4-8c5d-77fba06372f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: RMSE = 0.4609\n",
      "Fold 2: RMSE = 0.4650\n",
      "Fold 3: RMSE = 0.5508\n",
      "Fold 4: RMSE = 0.4733\n",
      "Fold 5: RMSE = 0.4824\n",
      "Average RMSE: 48.6500\n",
      "Root Mean Squared Error: 2238.1403217850307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000</td>\n",
       "      <td>203.609721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7001</td>\n",
       "      <td>156.940156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7002</td>\n",
       "      <td>70.767527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7003</td>\n",
       "      <td>181.066538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7004</td>\n",
       "      <td>210.815451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID       price\n",
       "0  7000  203.609721\n",
       "1  7001  156.940156\n",
       "2  7002   70.767527\n",
       "3  7003  181.066538\n",
       "4  7004  210.815451"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "    \n",
    "#df6  = pd.read_csv('df_6.csv')\n",
    "    \n",
    "df_train = df6[:7000]\n",
    "\n",
    "#df_train = df_train[df_train['price'] < 3001]\n",
    "#df_train = find_outliers(df_train)    \n",
    "# Remove Age outliers\n",
    "#price_outlier = get_outliers(df_train['price'])\n",
    "#df_train = df_train[~df_train['price'].isin(price_outlier['Outlier'].values)]\n",
    "#df_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "df_test = df6[7000:]\n",
    "    \n",
    "columns_to_drop = [\n",
    "'name','description','neighborhood_overview','host_name','host_location','host_about',\n",
    "'host_neighbourhood','neighbourhood','first_review','last_review',\n",
    "    \n",
    "#amenities'    \n",
    "  \n",
    "'price'\n",
    ",'log_price'\n",
    "    \n",
    "#,'property_type_Private room in home'\n",
    "#,'availability_30'\n",
    "    \n",
    ",'calculated_host_listings_count_entire_homes'\n",
    ",'calculated_host_listings_count_shared_rooms'\n",
    "    \n",
    "#,'maximum_nights_avg_ntm'\n",
    "#,'instant_bookable_t'\n",
    "#,'maximum_maximum_nights'\n",
    "#,'number_of_reviews_l30d'\n",
    "#,'minimum_minimum_nights'\n",
    "    \n",
    ",'review_scores_location'\n",
    ",'calculated_host_listings_count'\n",
    "    \n",
    ",'property_type_Entire home'\n",
    ",'maximum_minimum_nights'\n",
    ",'smoke_alarm'\n",
    ",'review_scores_rating'\n",
    ",'property_type_other'\n",
    ",'minimum_nights'\n",
    ",'availability_365'\n",
    ",'number_of_reviews_ltm'\n",
    ",'availability_60'\n",
    ",'source_previous scrape'\n",
    ",'review_scores_communication'\n",
    ",'review_scores_value'\n",
    ",'review_scores_cleanliness'\n",
    ",'availability_90'\n",
    ",'neighbourhood_cleansed_Port Phillip'\n",
    ",'reviews_per_month'\n",
    ",'longitude'\n",
    ",'minimum_maximum_nights'\n",
    ",'host_acceptance_rate'\n",
    ",'neighbourhood_cleansed_Yarra Ranges'\n",
    ",'minimum_nights_avg_ntm'\n",
    ",'maximum_nights'\n",
    ",'email'\n",
    ",'amenity_count'\n",
    ",'host_response_rate'\n",
    ",'review_scores_accuracy'\n",
    ",'number_of_reviews'\n",
    ",'host_listings_count'\n",
    ",'review_scores_checkin'\n",
    ",'neighbourhood_cleansed_Yarra'\n",
    ",'neighbourhood_cleansed_Stonnington'\n",
    ",'beds'\n",
    ",'property_type_Entire rental unit'\n",
    ",'work_email'\n",
    ",'latitude'\n",
    ",'hangers'\n",
    ",'host_since'\n",
    ",'host_is_superhost_t'\n",
    ",'kitchen'\n",
    ",'wifi'\n",
    ",'property_type_Private room in rental unit'\n",
    ",'host_response_time'\n",
    ",'host_has_profile_pic_t'\n",
    ",'essentials'\n",
    ",'host_identity_verified_t'\n",
    ",'phone'\n",
    ",'has_availability_t'\n",
    "    \n",
    "    \n",
    "]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "y_train = df_train['log_price'].values\n",
    "    \n",
    "X_train = df_train.drop(columns=columns_to_drop).values\n",
    "    \n",
    "X_test =  df_test.drop(columns=columns_to_drop).values\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300,400,500,600,700,800],  # Number of boosting stages to perform\n",
    "#    'learning_rate': [0.1, 0.05, 0.01],  # Learning rate shrinks the contribution of each tree\n",
    "    'min_samples_split': [2, 5, 10,20,50],\n",
    "    'max_depth': [3, 4, 5, 10, 15, 20, 25, 30,50 ]  # Maximum depth of each decision tree\n",
    "}\n",
    "    \n",
    "#gb_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "#grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs = 24)\n",
    "    \n",
    "#grid_search.fit(X_train, y_train)\n",
    "#best_params = grid_search.best_params_\n",
    "#print(best_params)\n",
    "    \n",
    "#pipe = make_pipeline(StandardScaler(),\n",
    "#                    RandomForestRegressor(random_state=42, **best_params))\n",
    "    \n",
    "#pipe = make_pipeline(StandardScaler(),\n",
    "#                     RandomForestRegressor(random_state=42, n_estimators=100 , max_depth=700, min_samples_split=50))\n",
    "    \n",
    "#pipe = make_pipeline(StandardScaler(),\n",
    "#                     RandomForestRegressor(random_state=42, n_estimators=100 , max_depth=700, min_samples_split=20,\n",
    "#                                          n_jobs=-1 ))\n",
    "# n_estimators=10\n",
    "    \n",
    "#pipe = make_pipeline(StandardScaler(),\n",
    "#                     RandomForestRegressor(random_state=42, n_estimators=100 , max_depth=1700,max_features=1,\n",
    "#                                          bootstrap=False,criterion='squared_error',\n",
    "#                                          n_jobs=-1\n",
    "#                                          ))\n",
    "    \n",
    "pipe = make_pipeline(StandardScaler(),\n",
    "                     RandomForestRegressor(random_state=42, n_estimators=10 , max_depth=700))\n",
    "          \n",
    "    \n",
    "#{'max_depth': 10, 'min_samples_split': 20, 'n_estimators': 700}    \n",
    "    \n",
    "    \n",
    "x = pipe.fit(X_train, y_train)\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = pipe.predict(X_train)\n",
    "y_pred_dollar = np.round(np.exp(y_pred))\n",
    "    \n",
    "y_train_dollar = np.round(np.exp(y_train))\n",
    "    \n",
    "    \n",
    "    \n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "# Print the RMSE scores for each fold\n",
    "for i, rmse in enumerate(rmse_scores):\n",
    "    print(\"Fold {}: RMSE = {:.4f}\".format(i+1, rmse))\n",
    "    \n",
    "# Calculate and print the average RMSE across all folds\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(\"Average RMSE: {:.4f}\".format(average_rmse*100))\n",
    "    \n",
    "    \n",
    "    \n",
    "rmse = np.sqrt(mean_squared_error(y_train_dollar, y_pred_dollar))\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "    \n",
    "train_pred = pd.DataFrame({\"price\":y_train_dollar, \"pred\":y_pred_dollar})\n",
    "train_pred.head(5)\n",
    "    \n",
    "    \n",
    "#df_test = pd.read_csv(\\\"test.csv\\\")\n",
    "#X_test = df_test.drop(['ID'], axis=1).values\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_pred_dollar = np.round(np.exp(y_pred))\n",
    "    \n",
    "#test_pred = pd.DataFrame({\"price\":y_test_dollar, \"pred\":y_test_pred_dollar})\n",
    "#test_pred.head(100)\n",
    "    \n",
    "    \n",
    "test_id = np.arange(7000, 10000, 1)\n",
    "pred = pd.DataFrame({\"ID\":test_id, \"price\": np.exp(y_pred)})\n",
    "pred.to_csv(\"pricepredictions.csv\", index=False, header=True)\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1010e5-03e6-475b-a9f8-51b19cb556d2",
   "metadata": {},
   "source": [
    "`(Task 3 - insert more cells as required)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
